# Conway's Game of Life - MPI Parallel Implementation

Distributed systems project implementing Conway's Game of Life using MPI for parallel computation with 1D row decomposition.

## ğŸš€ Quick Start

### Local Run (macOS)

```bash
# Setup environment
./setup_local.sh

# Run simulation (saves snapshots)
uv run mpirun -np 4 python life_mpi.py \
    --nx 512 --ny 512 \
    --steps 500 \
    --pattern glider_gun \
    --output-dir snapshots \
    --save-interval 50

# Create animation
uv run python visualize.py \
    --input-dir snapshots \
    --output animation.gif \
    --fps 10
```

### HPC Cluster (SLURM)

```bash
# Submit job
sbatch submit.sbatch

# After completion, create visualizations
python visualize.py \
    --input-dir snapshots \
    --output animation.gif
```

## ğŸ“ Project Structure

```
Core Files:
â”œâ”€â”€ life_mpi.py               # Main simulation (MPI parallelized)
â”œâ”€â”€ visualize.py              # Post-processing visualization
â”œâ”€â”€ submit.sbatch             # SLURM job submission script
â””â”€â”€ setup_local.sh            # Local environment setup

Testing & Benchmarking:
â”œâ”€â”€ test_correctness_local.sh # Verify correctness across different np
â”œâ”€â”€ benchmark_local.sh        # Local performance benchmarking
â””â”€â”€ benchmark_hpc.sh          # HPC performance benchmarking

Configuration:
â”œâ”€â”€ pyproject.toml            # Python dependencies
â”œâ”€â”€ uv.lock                   # Dependency lock file
â””â”€â”€ .python-version           # Python version (3.12)

Documentation:
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ USAGE.md                  # Detailed usage guide
```

## âœ¨ Features

âœ… **1D Row Decomposition** - Grid partitioned across MPI ranks by rows  
âœ… **Toroidal Boundaries** - Wrap-around edges (Pac-Man style)  
âœ… **Non-blocking Communication** - `Isend`/`Irecv` + `Waitall` for efficiency  
âœ… **Halo Exchange** - Automatic ghost cell communication  
âœ… **Binary Snapshots** - Compressed `.npz` format for fast I/O  
âœ… **Post-processing Visualization** - Separate from simulation for HPC  
âœ… **Rank Heatmap** - Visualize workload distribution across ranks  
âœ… **Correctness Testing** - Verify identical output across different process counts  
âœ… **Performance Benchmarking** - Measure speedup and parallel efficiency  

## ğŸ§ª Testing

### Correctness Test
```bash
./test_correctness_local.sh
```
Runs with `np=1,2,4,8` and verifies all produce identical checksums.

### Performance Benchmark
```bash
./benchmark_local.sh
```
Measures execution time, speedup, and efficiency. Generates plots.

## ğŸ“Š Example Results

### Small Grid (512Ã—512, 4 ranks)
- Time: ~0.5 seconds for 200 steps
- Speedup: 3.2Ã— (vs 1 rank)
- Efficiency: 80%

### Large Grid (16KÃ—16K, 32 ranks)
- Time: ~120 seconds for 500 steps
- Output: ~400MB snapshots (save-interval=100)

### Massive Grid (64KÃ—64K, 512 ranks)
- Feasible on HPC with proper resource allocation
- Snapshots saved for post-processing

## ğŸ¯ Patterns

- **`glider_gun`** - Gosper glider gun (generates gliders)
- **`glider`** - Simple moving pattern
- **`r_pentomino`** - Chaotic, long-lived pattern
- **`random`** - Random initial state

## ğŸ“– Documentation

See **`USAGE.md`** for:
- Detailed command-line options
- HPC SLURM scripts
- Visualization examples
- Troubleshooting guide
- Performance optimization tips

## ğŸ› ï¸ Technical Details

### MPI Operations Used:
- `COMM_WORLD` - Communicator
- `Get_rank()`, `Get_size()` - Process identification
- `Bcast()` - Broadcast grid dimensions
- `Scatterv()` - Distribute variable-sized chunks
- `Gatherv()` - Collect results
- `Isend()`, `Irecv()` - Non-blocking point-to-point
- `Waitall()` - Wait for all requests
- `Barrier()` - Synchronization

### Game of Life Rules (B3/S23):
- **Birth**: Dead cell with exactly 3 neighbors becomes alive
- **Survival**: Live cell with 2-3 neighbors stays alive
- **Death**: Otherwise, cell dies

### Architecture:
1. **Simulation** (`life_mpi.py`) - Fast, minimal overhead
2. **Visualization** (`visualize.py`) - Separate post-processing
3. **Testing** - Automated correctness and performance checks

## ğŸ”§ Requirements

- Python 3.12+
- `mpi4py` - MPI bindings
- `numpy` - Array operations
- `matplotlib` - Visualization (optional, for post-processing)
- `scipy` - Gaussian filter for heatmap (optional)
- `pillow` - GIF creation (optional)

## ğŸ“ Notes

- Optimized for large-scale HPC runs (64KÃ—64K grids)
- Simulation and visualization separated for efficiency
- All MPI communication is non-blocking to avoid deadlocks
- Toroidal boundaries ensure no edge effects
- Binary snapshots compressed with `numpy.savez_compressed`

## ğŸ“ Academic Context

Graduate-level Distributed Systems course project demonstrating:
- MPI parallelization techniques
- Domain decomposition strategies
- Performance analysis and optimization
- Scalability testing
- Load balancing visualization

---

**Author:** RRyas  
**Course:** Distributed Systems  
**Implementation:** 1D Row Decomposition with MPI
